---
layout: post
title:  "Price of Privacy in the Keynesian Beauty Contest"
date:   2019-4-30
tags: research, computer science, machine learning, fairness
mathjax: true
permalink: /blog/ec19-privacykbc
summary: >
    Hadi and I wrote a paper which will be in EC this year!
---

Last year, Hadi Elzayn and I took a course in the Economics department about beliefs and learning in game theory with Annie Liang.  
At the end of the class, we had to write a paper, and Hadi and I decided to look at what happens in a game when the players 
are worried that their action reveals some sensitive private information.  If players' response to this fact is to inject some random 
noise into their action, this will affect the players' utilities and the equilibrium of the game.  We wrote our paper and submitted it to 
EC and it was accepted!  This is my first paper with only student authors and checks off an academic bucket list item of publishing a course 
project (I guess this was kind of my last opportunity, since I'm also done with coursework!).

The game we look at is a variation of the *Keynesian Beauty Contest*.  In his *General Theory*, Keynes describes a game by way of analogy: 
a newspaper runs a beauty contest and prints pictures of the faces of 100 women (of course). Readers of the newspaper mail in with a selection 
of six faces.  The reader who wins is the one whose six choices *were the most popular choices*.  Therefore, in playing this game, an entrant 
needs to think about which faces they find the most beautiful, but also which faces everyone else might find the most beautiful. This is called the 
*first-order belief* about other players.  The player also needs to wonder about everyone else's first-order beliefs.  That is, what does everyone think 
everyone thinks about the faces.  These are, naturally, called the *second-order beliefs*.  Keynes posits that people don't generally go past the 
second-order of beliefs, but maybe the most clever among us might go up to a third- or fourth-order.  However, in theory, we do need to worry about 
these things and have to consider equilibria consistent with this infinite hierarchy of beliefs.

In the formalism of the game we work with, in the absence of the privacy concern, there are some nice results about the equilibria. Therefore, our task 
isn't to derive these from scratch but rather to see whether these results still exist in the modified setting.  It's possible that they don't, and 
by adding a little bit of noise, players can knock the original game off equilibrium in a really bad way.  Fortunately, we show that this isn't the case. 
We show that the trade-off from adding noise (gaining some privacy, losing some utility in the original game) has a nice form and there is an equilibrium 
where players add a particular amount of noise to their private information and then play as if that were their true value. Since everyone knows that 
everyone knows that everyone knows that (and so on) people are doing this, we can easily relate our equilibrium to the one in the original game.

As a bonus, we give derivations for all the results in the case where there are finitely many players, since the existing results (as is typical in economics) 
only exist where there are infinitely many agents.  You can check out the paper on the arXiv [here](https://arxiv.org/abs/1905.00844).